#-*-coding:utf-8-*-
import requests
import re
from bs4 import BeautifulSoup


category_dict = {
	'数据统计':'sjtj',
	'相关新闻':'xgxw',
	'通知公告':'tzgg',
	'中心管理制度':'zxglzd',
	'国家法规':'gjfg',
	'地方法规':'dffg',
	'行业标准':'hybz',
	'表格下载':'bgxz',
	'未分类':'wfl'
	}

### item模板
item0 = """
	<item>
		<title>{}</title>
		<!-- <pubDate>Sat, 18 Mar 1970 06:25:29 +0000</pubDate> -->
		<dc:creator><![CDATA[master]]></dc:creator>
		<description></description>
		<content:encoded><![CDATA[{}]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_date><![CDATA[{} 00:00:00]]></wp:post_date>
		<!-- <wp:post_date><![CDATA[2013-03-18 14:25:29]]></wp:post_date> -->
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="{}"><![CDATA[{}]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_vdw_gallery_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
	</item>
"""
item = ""


### 获取其中一个文章
url = 'http://selukwe.cn/da/?p='

i = 0
total = 0

for index in range(1,1301) :
	### 判断获取url
	r = requests.get(url+str(index))
	if r.status_code != 200 :
		print ('p='+str(index)+' -> 文章不存在')
		#continue
	else :
		print ('p='+str(index)+' -> 获取文章成功')
		total += 1
		### 开始获取所需信息
		# url正确，获取内容
		html_text = r.text
		### 开始搜索标题，title_list储存所有标题
		title_pattern = '<h5>.+</h5>'
		if (re.search(title_pattern,html_text)) == None :
			title = '该文章未撰写标题'
		else :
			match_title = re.search(title_pattern,html_text).group()
			l = len(match_title)
			title = match_title[4:l-5]
		title_list = []
		title_list.append(title)
		#print (title_list[0])
		### 开始搜索发布日期，time_list储存所有标题
		time_pattern = '<span class="article-date">.+</span>'
		time_list = []
		time_list0 = []
		if (re.search(time_pattern,html_text)) == None :
			time = '1970-01-01'
			time_list.append(time)
			time_list0.append(time)
		else :
			match_time = re.search(time_pattern,html_text).group()
			l = len(match_time)
			time = match_time[28:l-7]
			time_list.append(time)
		#print (time_list[i])
		### 开始输出发布日期（后台）
			time0 =time_list[i]
			time0 = time0[3:]
			time_list0.append(time0)
		#print (time_list0[i])
		### 开始搜索文章内容，content_list储存所有标题
		### 此处用BeautifulSoup而不用正则
		soup = BeautifulSoup(html_text, 'html.parser') # 初始化函数，变量 html_text 是我们获取的页面代码，第二个参数是固定的，表示识别html代码
		match_content = soup.select('.nr_text')[0] # 获取类为 nr_text 的内容,此处不需要进行".encode('utf-8')"处理
		content = match_content # 此处直接用带原class的文本
		#match_content = str(match_content)
		#l = len(content) # 此处soup的返回值是list类型，但是为什么长度是3？
		#print (l)
		#content = match_content[21:l-6]
		content_list = []
		content_list.append(content)
		#print (content_list[0])
		### 开始搜索文章归属分类目录，category_list储存所有文章唯一的分类目录
		#category_pattern = r'<h4>(.|\n)*?</h4>' # .*表示匹配最长的，.*?匹配最短的
		#match_category = re.match(category_pattern,html_text).group()
		#l = len(match_category)
		#category = match_category[4:l-5]
		#category_list = []
		#category_list.append(category)
		#print (match_category)
		#### 开始搜索文章归属分类目录，category_list储存所有文章唯一的分类目录
		category_pattern = r'<h4>(.|\n)*?</h4>'
		match_category = re.search(category_pattern,html_text).group()
		#print (match_category) #带有换行、空格和html标签的分类目录
		match_category = ''.join(match_category.split())
		#print (match_category)带html标签的分类目录
		l = len(match_category)
		category = match_category[4:l-5] # 文字分类目录
		category_list = []
		category_list.append(category)
		### 分类目录别名
		if category in category_dict:
			#print (category_dict[category])
			nicename = category_dict[category]
		nicename_list = []
		nicename_list.append(nicename)
		### 叠加item模板
		#item = item.format(title_list[0],time_list[0],content_list[0])
		item += item0.format(title_list[i],content_list[i],time_list0[i],nicename_list[i],category_list[i])



demo = """
			<?xml version="1.0" encoding="UTF-8" ?>
			<!-- This is a WordPress eXtended RSS file generated by WordPress as an export of your site. -->
			<!-- It contains information about your site's posts, pages, comments, categories, and other content. -->
			<!-- You may use this file to transfer that content from one site to another. -->
			<!-- This file is not intended to serve as a complete backup of your site. -->

			<!-- To import this information into a WordPress site follow these steps: -->
			<!-- 1. Log in to that site as an administrator. -->
			<!-- 2. Go to Tools: Import in the WordPress admin panel. -->
			<!-- 3. Install the "WordPress" importer from the list. -->
			<!-- 4. Activate & Run Importer. -->
			<!-- 5. Upload this file using the form provided on that page. -->
			<!-- 6. You will first be asked to map the authors in this export file to users -->
			<!--    on the site. For each author, you may choose to map to an -->
			<!--    existing user on the site or to create a new user. -->
			<!-- 7. WordPress will then import each of the posts, pages, comments, categories, etc. -->
			<!--    contained in this file into your site. -->

			<!-- generator="WordPress/4.7.6" created="2017-10-18 12:35" -->
			<rss version="2.0"
				xmlns:excerpt="http://wordpress.org/export/1.2/excerpt/"
				xmlns:content="http://purl.org/rss/1.0/modules/content/"
				xmlns:wfw="http://wellformedweb.org/CommentAPI/"
				xmlns:dc="http://purl.org/dc/elements/1.1/"
				xmlns:wp="http://wordpress.org/export/1.2/"
			>

			<channel>
				<language>zh-CN</language>
				<wp:wxr_version>1.2</wp:wxr_version>
				{}
			</channel>
			</rss>
		"""

demo = demo.format(item)

### 编写demo.xml函数
with open ('fosuda.xml','w', encoding='utf-8') as demof :
	demof.write(demo)
	### 此处不需要加判断是否关闭文件
	# print ('是否处于关闭状态：', demo.closed)

print ("对http://selukwe.cn/da/?p=index共获取"+str(total)+"篇文章")
print ("导出xml文件成功")





### 这样为逐行输出，但本次用全部一次性输出
# with open ('demo.xml','r', encoding='utf-8') as demo :
#	for line in demo.readlines():
#		# 把末尾的'\n'删掉
#		print (line.strip())
### 这样为全部一次性输出
# with open ('demo.xml','r+', encoding='utf-8') as demo :
#	print (demo.read())
# demo_read()





